Test two extremes and middle ground of the conservative subproblem approach:

- One step per batch (parameter update) either by SGD/first-order-variant
  or Second-order method. Step acceptence might already be important for 
  deep/recurrent models

- Middle-ground: Multiple steps of first or second order method per batch
  (batch persistence). Modifications to SGD with step acceptance parameter 
  start being more relevant in this regime

- Full optimization for each batch, cumulative averaging over batches
  or other averaging technique

spectrum should go:

Time (computation) efficient <----> Data efficient
       L = 1                           L = oo

What methods perform better at what points in the spectrum?
Can second order methods @ L = oo outperform first order @ L = 0

BATCH_SIZE plays a huge role for the deterministic setting
there seems to be a marginal value after which training on a single batch is valuable
and won't overfit.
As such there may be two important regimes where B > or < B_critical
for B < B_critical a naive algoithm may not work easily but this is probably
the most relevant setting for most ML tasks handled by Deep Learning

----------------------------------------------------------------------------

Can solve problems in the regime L = oo, B > B_critical (deterministic with enough data for learning) with 2nd order methods
Can solve problems in the regime L ~ 1, B = Any (standard stochastic setting) with first order methods
Solve problems in between these regimes???

Look into natural gradient and the Fisher Information
